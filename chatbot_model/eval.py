import torch
from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel
from fastapi import FastAPI, APIRouter, HTTPException
from pydantic import BaseModel
import os

processing_router1 = APIRouter()

class ChatRequest(BaseModel):
    message: str

ME_TKN = "<me>" #user
YOU_TKN = "<you>" #bot
BOS = "</s>" #begin sentence
EOS = "</s>" #end sentence
MASK = "<mask>"
SENT = "<sent>" #ë¬¸ì¥ êµ¬ë¶„
PAD = "<pad>" #íŒ¨ë”©

tokenizer = PreTrainedTokenizerFast.from_pretrained(
    "skt/kogpt2-base-v2",
    bos_token=EOS,
    eos_token=EOS,
    unk_token="<unk>",
    pad_token=PAD,
    mask_token=MASK,
)

special_tokens = [ME_TKN, YOU_TKN, SENT]
tokenizer.add_tokens(special_tokens)

# repo_id = "louisan1128/chatanalysis"
model_path = "/app/resources/finetuned"

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# model = GPT2LMHeadModel.from_pretrained(model_path).to(device)
# model.eval()
model = None



# ì±—ë´‡ ì‘ë‹µ ìƒì„± í•¨ìˆ˜
@processing_router1.post("/evaluate")
def generate_response(req: ChatRequest, max_len=100, top_p=0.9, top_k=50):
    global model
    if model is None:
        print("ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        required_files = ['config.json']

        for f in required_files:
            if not os.path.exists(os.path.join(model_path, f)):
                raise HTTPException(
                    status_code=503,
                    detail=f"ëª¨ë¸ íŒŒì¼ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ({f} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤)"
                )
        print(f"ëª¨ë¸ íŒŒì¼ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. {model_path}ì—ì„œ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.")
        try:
            model = GPT2LMHeadModel.from_pretrained(model_path).to(device)
            model.eval()
            print("ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            raise HTTPException(
                status_code=500,
                detail=f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            )
        
    print(req.message)
    input_text = f"{ME_TKN}{req.message}{SENT}{YOU_TKN}"
    input_ids = tokenizer.encode(input_text, return_tensors="pt").to(device)

    output = model.generate(
        input_ids,
        max_length=len(input_ids[0]) + max_len,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
        do_sample=True,
        top_k=top_k,
        top_p=top_p,
        temperature=0.8,
        repetition_penalty=1.5,
    )

    response = tokenizer.decode(output[0], skip_special_tokens=False)
    
    you_index = response.find(YOU_TKN)
    if you_index != -1:
        response = response[you_index + len(YOU_TKN):]

    for tok in [ME_TKN, YOU_TKN, SENT, PAD, MASK, EOS]:
        response = response.replace(tok, "")


    return {"response": response}


### ì˜ˆì‹œ
# if __name__ == "__main__":
#     print("ğŸ¤– KoGPT2 ì±—ë´‡ (ë‹¨ë°œì„± ëŒ€í™”) ì‹œì‘! (ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ì…ë ¥)")

#     while True:
#         user_input = input("ğŸ‘¤ You: ")
#         if user_input.lower() in ["quit", "exit", "ì¢…ë£Œ"]:
#             print("ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
#             break
        
#         user_input = f"{ME_TKN}{user_input}{SENT}{YOU_TKN}"
#         answer = generate_response(user_input)
#         print(f"ğŸ¤– Bot:{answer}")
